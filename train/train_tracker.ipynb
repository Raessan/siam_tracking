{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322ce283-943e-4b01-9e3f-5275cb1e9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset_lasot import DatasetLaSOT\n",
    "from src.model import SiameseTracker\n",
    "from src.loss import compute_loss\n",
    "from src.draw_samples_training import draw_samples_training\n",
    "import config.config as cfg\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f5fb52-84c9-4b5a-9061-e8e452ba790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafa/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rafa/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded!\n",
      "Freezing: ['stem', 'layer1', 'layer2']\n",
      "Total number of parameters:  32674883\n",
      "Total number of trainable parameters:  31229955\n",
      "Number parameters backbone:  8543296\n",
      "Number parameters cross attn:  4198400\n",
      "Number parameters classification:  9441281\n",
      "Number parameters regression:  9442306\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "################ LOAD ALL THE PARAMETERS #############################\n",
    "# DATASET PARAMETERS\n",
    "DIR_DATA = cfg.DIR_DATA # Root to the folder with the prepared data\n",
    "SIZE_TEMPLATE = cfg.SIZE_TEMPLATE\n",
    "SIZE_SEARCH = cfg.SIZE_SEARCH\n",
    "SIZE_OUT = cfg.SIZE_OUT\n",
    "MAX_FRAME_SEP = cfg.MAX_FRAME_SEP\n",
    "NEG_PROB = cfg.NEG_PROB\n",
    "EXTRA_CONTEXT_TEMPLATE = cfg.EXTRA_CONTEXT_TEMPLATE\n",
    "MIN_EXTRA_CONTEXT_SEARCH = cfg.MIN_EXTRA_CONTEXT_SEARCH\n",
    "MAX_EXTRA_CONTEXT_SEARCH = cfg.MAX_EXTRA_CONTEXT_SEARCH\n",
    "MAX_SHIFT = cfg.MAX_SHIFT\n",
    "REG_FULL = cfg.REG_FULL\n",
    "IMG_AUGMENT_TRAINING = cfg.IMG_AUGMENT_TRAINING\n",
    "IMG_AUGMENT_VALID = cfg.IMG_AUGMENT_VALID\n",
    "IMG_MEAN = cfg.IMG_MEAN\n",
    "IMG_STD = cfg.IMG_STD\n",
    "    \n",
    "# MODEL PARAMETERS\n",
    "BATCH_SIZE = cfg.BATCH_SIZE\n",
    "LAYERS_FREEZE = cfg.LAYERS_FREEZE\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "THRESHOLD_CLS = cfg.THRESHOLD_CLS\n",
    "ALPHA_LOSS = cfg.ALPHA_LOSS\n",
    "GAMMA_LOSS = cfg.GAMMA_LOSS \n",
    "WEIGHT_LOSS = cfg.WEIGHT_LOSS\n",
    "\n",
    "LEARNING_RATE = cfg.LEARNING_RATE\n",
    "NUM_EPOCHS = cfg.NUM_EPOCHS\n",
    "NUM_SAMPLES_PLOT = cfg.NUM_SAMPLES_PLOT\n",
    "\n",
    "LOAD_MODEL = cfg.LOAD_MODEL\n",
    "SAVE_MODEL = cfg.SAVE_MODEL\n",
    "MODEL_PATH_TRAIN_LOAD = cfg.MODEL_PATH_TRAIN_LOAD\n",
    "RESULTS_PATH = cfg.RESULTS_PATH\n",
    "\n",
    "train_set = DatasetLaSOT(\"train\", DIR_DATA, SIZE_TEMPLATE, SIZE_SEARCH, SIZE_OUT, MAX_FRAME_SEP, \n",
    "                                 NEG_PROB, EXTRA_CONTEXT_TEMPLATE, MIN_EXTRA_CONTEXT_SEARCH, MAX_EXTRA_CONTEXT_SEARCH, MAX_SHIFT, REG_FULL,\n",
    "                                IMG_AUGMENT_TRAINING, IMG_MEAN, IMG_STD)\n",
    "train_dataloader = DataLoader(train_set, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_set = DatasetLaSOT(\"val\", DIR_DATA, SIZE_TEMPLATE, SIZE_SEARCH, SIZE_OUT, MAX_FRAME_SEP, \n",
    "                                 NEG_PROB, EXTRA_CONTEXT_TEMPLATE, MIN_EXTRA_CONTEXT_SEARCH, MAX_EXTRA_CONTEXT_SEARCH, MAX_SHIFT, REG_FULL,\n",
    "                                  IMG_AUGMENT_VALID, IMG_MEAN, IMG_STD)\n",
    "val_dataloader = DataLoader(val_set, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = SiameseTracker(SIZE_TEMPLATE, SIZE_SEARCH, SIZE_OUT, REG_FULL).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Load model\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH_TRAIN_LOAD))\n",
    "    print(\"Model successfully loaded!\")\n",
    "\n",
    "# Freeze parameters\n",
    "if LAYERS_FREEZE > 0:\n",
    "    children_backbone = list(model.backbone.named_children())\n",
    "    # 2) Take the first N names\n",
    "    to_freeze = [name for name, _ in children_backbone[:LAYERS_FREEZE]]\n",
    "    print(\"Freezing:\", to_freeze)\n",
    "    \n",
    "    # 3) Freeze all parameters whose name starts with those modules\n",
    "    for name, param in model.backbone.named_parameters():\n",
    "        if any(name.startswith(layer) for layer in to_freeze):\n",
    "            param.requires_grad = False\n",
    "\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"Total number of parameters: \", n_params)\n",
    "n_trainable_params = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "print(\"Total number of trainable parameters: \", n_trainable_params)\n",
    "n_params_backbone = sum([p.numel() for p in model.backbone.parameters()])\n",
    "print(\"Number parameters backbone: \", n_params_backbone)\n",
    "n_params_cross_attn = sum([p.numel() for p in model.cross_attn.parameters()])\n",
    "print(\"Number parameters cross attn: \", n_params_cross_attn)\n",
    "n_params_cls = sum([p.numel() for p in model.cls_head.parameters()])\n",
    "print(\"Number parameters classification: \", n_params_cls)\n",
    "n_params_reg = sum([p.numel() for p in model.reg_head.parameters()])\n",
    "print(\"Number parameters regression: \", n_params_reg)\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    folder_path = f\"{RESULTS_PATH}/{current_date}\"\n",
    "    \n",
    "    json_params = { \n",
    "        \"SIZE_TEMPLATE\" : SIZE_TEMPLATE, \n",
    "        \"SIZE_SEARCH\" : SIZE_SEARCH, \n",
    "        \"SIZE_OUT\" : SIZE_OUT, \n",
    "        \"MAX_FRAME_SEPT\" : MAX_FRAME_SEP,\n",
    "        \"NEG_PROB\" : NEG_PROB,\n",
    "        \"EXTRA_CONTEXT_TEMPLATE\" : EXTRA_CONTEXT_TEMPLATE,\n",
    "        \"MIN_EXTRA_CONTEXT_SEARCH\" : MIN_EXTRA_CONTEXT_SEARCH,\n",
    "        \"MAX_EXTRA_CONTEXT_SEARCH \" : MAX_EXTRA_CONTEXT_SEARCH,\n",
    "        \"MAX_SHIFT\" : MAX_SHIFT,\n",
    "        \"REG_FULL\" : REG_FULL,\n",
    "        \"IMG_AUGMENT_TRAINING\": IMG_AUGMENT_TRAINING,\n",
    "        \"IMG_AUGMENT_VALID\": IMG_AUGMENT_VALID,\n",
    "        \"THRESHOLD_CLS\" : THRESHOLD_CLS,\n",
    "        \"ALPHA_LOSS\" : ALPHA_LOSS,\n",
    "        \"GAMMA_LOSS\" : GAMMA_LOSS,\n",
    "        \"WEIGHT_LOSS\" : WEIGHT_LOSS,\n",
    "        \"LEARNING_RATE\" : LEARNING_RATE,\n",
    "        \"LOAD_MODEL\" : LOAD_MODEL,\n",
    "        \"MODEL_PATH_TRAIN_LOAD\" : MODEL_PATH_TRAIN_LOAD,\n",
    "        \"LAYERS_FREEZE\": LAYERS_FREEZE\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabf7a5c-64ed-4284-b26e-4e894fed84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                       | 2/79025 [00:01<12:13:51,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (template, search, heatmap, bbox, video_template_name, video_search_name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataloader)):\n\u001b[1;32m      7\u001b[0m     template, search, heatmap, bbox \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), search\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), heatmap\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat), bbox\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/siam_tracking/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/deep_learning/projects/siam_tracking/src/dataset_lasot.py:408\u001b[0m, in \u001b[0;36mDatasetLaSOT.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    406\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphotometric_augment(template)\n\u001b[1;32m    407\u001b[0m     search \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphotometric_augment(search)\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_tensor(template, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd), \u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m, heatmap, reg_wh, video_name, video_search_name\n",
      "File \u001b[0;32m~/deep_learning/projects/siam_tracking/src/utils.py:76\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(img, mean, std)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mConverts an img to a tensor ready to be used in NN\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[0;32m---> 76\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##################### TRAIN #######################\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (template, search, heatmap, bbox, video_template_name, video_search_name) in enumerate(tqdm(train_dataloader)):\n",
    "        template, search, heatmap, bbox = template.to(device, dtype=torch.float), search.to(device, dtype=torch.float), heatmap.to(device, dtype=torch.float), bbox.to(device, dtype=torch.float)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred_heatmap, pred_bbox = model(template, search)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(pred_heatmap, pred_bbox, heatmap, bbox, ALPHA_LOSS, GAMMA_LOSS, WEIGHT_LOSS)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss[0].backward()\n",
    "        \n",
    "        # Gradient clipping and Optimize\n",
    "        # clip all gradients to max norm 5.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss[0].item()\n",
    "\n",
    "        if (batch_idx % 1000 == 0 and batch_idx > 0):\n",
    "            print(f\"Epoch {epoch+1}, batch {batch_idx}, Loss: {train_loss/(batch_idx+1)}, cls loss: {loss[1].item()}, regression loss: {loss[2].item()}\")\n",
    "\n",
    "        if (batch_idx % 10000 == 0 and batch_idx > 0):\n",
    "            draw_samples_training(template, search, torch.sigmoid(pred_heatmap), pred_bbox, heatmap, bbox, train_set.mean, train_set.std, THRESHOLD_CLS, NUM_SAMPLES_PLOT, video_template_name, video_search_name)\n",
    "    \n",
    "    train_loss /= float(batch_idx+1)\n",
    "    \n",
    "    ##################### VALIDATION #######################\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loss_cls = 0.0\n",
    "    val_loss_reg = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (template, search, heatmap, bbox, video_template_name, video_search_name) in enumerate(tqdm(val_dataloader)):\n",
    "            template, search, heatmap, bbox = template.to(device, dtype=torch.float), search.to(device, dtype=torch.float), heatmap.to(device, dtype=torch.float), bbox.to(device, dtype=torch.float)\n",
    "    \n",
    "            # Forward pass\n",
    "            pred_heatmap, pred_bbox = model(template, search)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = compute_loss(pred_heatmap, pred_bbox, heatmap, bbox, ALPHA_LOSS, GAMMA_LOSS, WEIGHT_LOSS)\n",
    "    \n",
    "            val_loss += loss[0].item()\n",
    "            val_loss_cls += loss[1].item()\n",
    "            val_loss_reg += loss[2].item()\n",
    "    \n",
    "            if (batch_idx == 0):\n",
    "                draw_samples_training(template, search, torch.sigmoid(pred_heatmap), pred_bbox, heatmap, bbox, train_set.mean, train_set.std, THRESHOLD_CLS, NUM_SAMPLES_PLOT, video_template_name, video_search_name)\n",
    "    \n",
    "        val_loss /= float(batch_idx+1)\n",
    "        val_loss_cls /= float(batch_idx+1)\n",
    "        val_loss_reg /= float(batch_idx+1)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss}, val loss NN total: {val_loss}, val loss CLS: {val_loss_cls},  val loss Reg: {val_loss_reg}\")\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            # Save model and params\n",
    "            json_params_epoch = json_params.copy()\n",
    "            json_params_epoch[\"epoch\"] = epoch\n",
    "            json_params_epoch[\"train_loss\"] = train_loss\n",
    "            json_params_epoch[\"val_loss\"] = val_loss\n",
    "            json_params_epoch[\"val_loss_cls\"] = val_loss_cls\n",
    "            json_params_epoch[\"val_loss_reg\"] = val_loss_reg\n",
    "            model_path = os.path.join(folder_path,f\"model_{epoch}.pth\")\n",
    "            json_path = os.path.join(folder_path,f\"params_{epoch}.json\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            with open(json_path, \"w\") as outfile:\n",
    "                json.dump(json_params_epoch, outfile)\n",
    "    \n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
