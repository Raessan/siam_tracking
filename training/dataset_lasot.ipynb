{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6996d22-9821-43c0-b84d-5b26c44d852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from utils import *\n",
    "from augmentation import Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b88d458-b0cd-44a6-b370-79424b9a0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLaSOT(Dataset):\n",
    "    def __init__(self, mode, dir_data, size_template, size_search, size_out, max_frame_sep, neg_prob=0.5, extra_context_template=0.5, min_extra_context_search=0.75, max_extra_context_search=1.0, max_shift=0):\n",
    "        self.mode = mode\n",
    "        self.dir_data = dir_data\n",
    "        self.size_template = size_template\n",
    "        self.size_search = size_search\n",
    "        self.size_out = size_out\n",
    "        self.neg_prob = neg_prob\n",
    "        self.max_frame_sep = max_frame_sep\n",
    "        self.extra_context_template = extra_context_template\n",
    "        self.min_extra_context_search = min_extra_context_search\n",
    "        self.max_extra_context_search = max_extra_context_search\n",
    "        self.max_shift = max_shift\n",
    "        # mean/std for ImageNet‐pretrained backbones\n",
    "        # Adapt these variables to the backbone used\n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)[None,:,None,None]\n",
    "        self.std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)[None,:,None,None]\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            file = os.path.join(dir_data, \"training_set.txt\")\n",
    "        elif self.mode == \"test\":\n",
    "            file = os.path.join(dir_data, \"testing_set.txt\")\n",
    "        else:\n",
    "            raise Exception(\"the mode must be either train or test\")\n",
    "\n",
    "        # Get number of videos for the training set\n",
    "        with open(file, 'r') as file:\n",
    "            self.video_names = [line.strip() for line in file]\n",
    "\n",
    "        # Get the category names\n",
    "        self.categories = sorted([name for name in os.listdir(self.dir_data)\n",
    "              if os.path.isdir(os.path.join(self.dir_data, name))])\n",
    "\n",
    "        # Get the location of each frame per video and the bounding boxes (decided to keep as separate dictionaries\n",
    "        self.dict_frames_per_video = {}\n",
    "        self.dict_bboxes_per_video = {}\n",
    "        for video_name in self.video_names:\n",
    "            category = video_name.split('-')[0]\n",
    "            # This try is for the moment because I haven't downloaded the whole dataset. REMOVE\n",
    "            self.dict_frames_per_video[video_name] = sorted([os.path.join(dir_data, category, video_name, \"img\", frame) for frame in os.listdir(os.path.join(dir_data, category, video_name, \"img\"))])\n",
    "            self.dict_bboxes_per_video[video_name] = []\n",
    "            with open(os.path.join(dir_data, category, video_name, \"groundtruth.txt\"), \"r\") as f:\n",
    "                for line in f:\n",
    "                    bbox = list(map(int, line.strip().split(\",\")))\n",
    "                    self.dict_bboxes_per_video[video_name].append(bbox)\n",
    "\n",
    "\n",
    "        # Get the number of frames per video\n",
    "        self.dict_n_frames_per_video = {}\n",
    "        for key, value in self.dict_frames_per_video.items():\n",
    "            self.dict_n_frames_per_video[key] = len(value)\n",
    "\n",
    "        # Total number of frames\n",
    "        self.total_n_frames = sum(self.dict_n_frames_per_video.values())\n",
    "\n",
    "        # List of frames\n",
    "        #self.list_frames = []\n",
    "        #for key in sorted(self.dict_frames_per_video.keys()):\n",
    "            #self.list_frames.extend(self.dict_frames_per_video[key])\n",
    "\n",
    "    def get_data_from_idx(self, idx):\n",
    "        cumulative = 0\n",
    "        for key in sorted(self.dict_n_frames_per_video):\n",
    "            value = self.dict_n_frames_per_video[key]\n",
    "            if idx < cumulative + value:\n",
    "                return key, idx-cumulative\n",
    "            cumulative += value\n",
    "        raise IndexError(\"Index out of range\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_n_frames\n",
    "        \n",
    "    \n",
    "    def visualize_video(self, video_name, with_bboxes=True, fps=30):\n",
    "        frame_delay_ms = int(1000/fps)\n",
    "        for n_frame, frame_path in enumerate(self.dict_frames_per_video[video_name]):\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is None:\n",
    "                print(f\"Could not read {frame_path}\")\n",
    "                continue\n",
    "\n",
    "            if with_bboxes:\n",
    "                x, y, w, h = self.dict_bboxes_per_video[video_name][n_frame]\n",
    "                # Draw rectangle: image, top-left, bottom-right, color (BGR), thickness\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                \n",
    "            cv2.imshow(\"Video Playback\", frame)\n",
    "\n",
    "            if cv2.waitKey(frame_delay_ms) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    def get_context_bbox(self, bbox, extra_context):\n",
    "        \"\"\"\n",
    "        Convert a tight bbox (x, y, w, h) into a square context bbox with margin.\n",
    "        Returns (cx, cy, size).\n",
    "        \"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        cx = x + w / 2.\n",
    "        cy = y + h / 2.\n",
    "        # context padding = (w+h)*extra_context\n",
    "        pad = (w + h) * extra_context\n",
    "        # square size\n",
    "        size = np.sqrt((w + pad) * (h + pad))\n",
    "        return cx, cy, size\n",
    "\n",
    "    def crop_and_resize(self,\n",
    "                    frame,\n",
    "                    cx, cy,\n",
    "                    size,\n",
    "                    out_size,\n",
    "                    shift_x=0, shift_y=0):\n",
    "        \"\"\"\n",
    "        Crop a square patch of side 'size' centered at (cx, cy) from frame,\n",
    "        apply a shift in the out_size coordinate system, pad with border\n",
    "        replication if needed, and resize to (out_size, out_size).\n",
    "    \n",
    "        shift_x, shift_y: pixel offsets **in the resized patch**. Can be\n",
    "        positive or negative, moving the target around in the crop.\n",
    "        Returns the patch and the scaling factor.\n",
    "        \"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "    \n",
    "        # 1) convert shifts from output coords → original-frame coords\n",
    "        #    scale = out_size / size  ⇒  size/out_size = 1/scale\n",
    "        shift_x_orig = shift_x * size / out_size\n",
    "        shift_y_orig = shift_y * size / out_size\n",
    "    \n",
    "        # 2) adjust the true crop-center in the original frame\n",
    "        cx = cx + shift_x_orig\n",
    "        cy = cy + shift_y_orig\n",
    "    \n",
    "        # 3) now compute the square-window coords as before\n",
    "        x1 = cx - size/2\n",
    "        y1 = cy - size/2\n",
    "        x2 = x1 + size\n",
    "        y2 = y1 + size\n",
    "    \n",
    "        # 4) compute padding amounts for out‑of‑bounds regions\n",
    "        left   = int(max(0, -np.floor(x1)))\n",
    "        top    = int(max(0, -np.floor(y1)))\n",
    "        right  = int(max(0, np.ceil(x2)  - w))\n",
    "        bottom = int(max(0, np.ceil(y2)  - h))\n",
    "    \n",
    "        # 5) pad & crop\n",
    "        padded = cv2.copyMakeBorder(\n",
    "            frame,\n",
    "            top, bottom, left, right,\n",
    "            borderType=cv2.BORDER_REPLICATE\n",
    "        )\n",
    "        x1p, y1p = x1 + left,  y1 + top\n",
    "        x2p, y2p = x2 + left,  y2 + top\n",
    "        patch    = padded[int(y1p):int(y2p), int(x1p):int(x2p)]\n",
    "    \n",
    "        # 6) resize & return\n",
    "        patch_resized = cv2.resize(patch, (out_size, out_size))\n",
    "        scale = out_size / size\n",
    "        return patch_resized, scale\n",
    "\n",
    "    def preprocess_pair(self, frame1, frame2, bbox1, bbox2):\n",
    "        \"\"\"\n",
    "        Given two frames and their tight bboxes, compute exemplar & search patches,\n",
    "        along with their resized bbox coordinates in patch space.\n",
    "        Returns:\n",
    "          exemplar_img, search_img, exemplar_box, search_box\n",
    "        where boxes are (x, y, w, h) in the resized patch coordinate system.\n",
    "        \n",
    "        \"\"\"\n",
    "        extra_context_search = random.uniform(self.min_extra_context_search, self.max_extra_context_search)\n",
    "        print(\"Extra context search: \" + str(extra_context_search))\n",
    "        shift_x = random.randint(-self.max_shift, self.max_shift)\n",
    "        shift_y = random.randint(-self.max_shift, self.max_shift)\n",
    "        \n",
    "        # Frame 1: exemplar\n",
    "        cx1, cy1, size1 = self.get_context_bbox(bbox1, self.extra_context_template)\n",
    "        exemplar, scale1 = self.crop_and_resize(frame1, cx1, cy1, size1, self.size_template, 0, 0)\n",
    "    \n",
    "        # bbox1 in exemplar coords: centered\n",
    "        ex_bbox = [ (self.size_template - bbox1[2]*scale1)/2,\n",
    "                    (self.size_template - bbox1[3]*scale1)/2,\n",
    "                    bbox1[2]*scale1,\n",
    "                    bbox1[3]*scale1 ]\n",
    "    \n",
    "        # Frame 2: search\n",
    "        cx2, cy2, size2 = self.get_context_bbox(bbox2, extra_context_search)\n",
    "        search, scale2 = self.crop_and_resize(frame2, cx2, cy2, size2, self.size_search, shift_x, shift_y)\n",
    "    \n",
    "        # bbox2 in search coords before augment: centered\n",
    "        sr_bbox = [ (self.size_search - bbox2[2]*scale2)/2 - shift_x,\n",
    "                    (self.size_search - bbox2[3]*scale2)/2 - shift_y,\n",
    "                    bbox2[2]*scale2,\n",
    "                    bbox2[3]*scale2 ]\n",
    "    \n",
    "        return exemplar, search, ex_bbox, sr_bbox\n",
    "\n",
    "    def to_tensor(self, img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.\n",
    "        img = (img[None].transpose(0,3,1,2) - self.mean) / self.std\n",
    "        return torch.from_numpy(img[0])\n",
    "\n",
    "    def get_negative_sample(self, video_name_first_frame):\n",
    "        # Decide if the negative sample will have an object or pure background\n",
    "        if random.random() < 0.5:\n",
    "            print(\"With object!\")\n",
    "            with_object = True\n",
    "        else:\n",
    "            print(\"Without object!\")\n",
    "            with_object = False\n",
    "        # Select a video to sample from\n",
    "        video_name_second_frame = video_name_first_frame\n",
    "        while video_name_second_frame == video_name_first_frame:\n",
    "            video_name_second_frame = random.choice(list(self.dict_n_frames_per_video.keys()))\n",
    "        print(video_name_second_frame)\n",
    "        # Random frame\n",
    "        idx_second_frame = random.randint(0, self.dict_n_frames_per_video[video_name_second_frame] -1)\n",
    "        second_frame = cv2.imread(self.dict_frames_per_video[video_name_second_frame][idx_second_frame])\n",
    "        # Get the bounding box of the object\n",
    "        bbox2 = self.dict_bboxes_per_video[video_name_second_frame][idx_second_frame]\n",
    "        if with_object:\n",
    "            # We return directly the new frame and Bounding box\n",
    "            return second_frame, bbox2\n",
    "        else: # Get a random patch of the image. For that, we are going to move randomly the bbox center and apply the same logic\n",
    "            w, h = bbox2[2], bbox2[3]\n",
    "            # Make sure we are within the limits\n",
    "            max_x = second_frame.shape[1] - w\n",
    "            max_y = second_frame.shape[0] - h\n",
    "            x = random.randint(0,max_x)\n",
    "            y = random.randint(0,max_y)\n",
    "            return second_frame, [x, y, w, h]\n",
    "\n",
    "    def make_rect_tent(self, bbox):\n",
    "        \"\"\"\n",
    "        bbox = [xmin, ymin, w, h] in search-patch pixel coords.\n",
    "        Returns heatmap of shape (H,W), peak=1 at center,\n",
    "        linearly decaying to 0 at the box edges.\n",
    "        \"\"\"\n",
    "        stride = self.size_search / self.size_out\n",
    "    \n",
    "        # 1) cell centers in pixel coords\n",
    "        coords = np.arange(self.size_out) * stride + stride/2\n",
    "        xs, ys = np.meshgrid(coords, coords)  # (H,W)\n",
    "    \n",
    "        xmin, ymin, w, h = bbox\n",
    "        cx = xmin + w/2\n",
    "        cy = ymin + h/2\n",
    "    \n",
    "        # 2) normalized distances in [0,1]\n",
    "        dx = np.abs(xs - cx) / (w/2)\n",
    "        dy = np.abs(ys - cy) / (h/2)\n",
    "    \n",
    "        # 3) clamp and compute tent\n",
    "        tx = np.clip(1 - dx, 0, 1)\n",
    "        ty = np.clip(1 - dy, 0, 1)\n",
    "        heatmap = tx * ty  # (H,W)\n",
    "        # Force max to 1\n",
    "        heatmap /= heatmap.max()\n",
    "\n",
    "        # 4) build (w,h) regression map\n",
    "        mask = (heatmap > 0).astype(np.float32)      # (H,W)\n",
    "        reg_wh = np.zeros((self.size_out, self.size_out, 2), dtype=np.float32)\n",
    "        reg_wh[..., 0] = w/self.size_search * mask  # normalized width\n",
    "        reg_wh[..., 1] = h/self.size_search * mask  # normalized height\n",
    "\n",
    "        return heatmap, reg_wh\n",
    "\n",
    "    def get_positive_sample(self, video_name, idx_first_frame):\n",
    "        # Obtain the second idx and image\n",
    "        idx_second_frame = idx_first_frame\n",
    "        min_frame = max(0, idx_first_frame-self.max_frame_sep)\n",
    "        max_frame = min(self.dict_n_frames_per_video[video_name], idx_first_frame+self.max_frame_sep)\n",
    "        while idx_second_frame == idx_first_frame:\n",
    "            #idx_second_frame = random.choice(range(self.dict_n_frames_per_video[video_name]))\n",
    "            idx_second_frame = random.choice(range(min_frame, max_frame))\n",
    "            \n",
    "        second_frame = cv2.imread(self.dict_frames_per_video[video_name][idx_second_frame])\n",
    "\n",
    "        # Obtain bounding boxes\n",
    "        bbox2 = self.dict_bboxes_per_video[video_name][idx_second_frame]\n",
    "        return second_frame, bbox2\n",
    "\n",
    "    #def get_output(self, search_img, search_bbox):\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Returns the inputs and output for the learning problem. The input\n",
    "        consists of an reference image tensor and a search image tensor, the\n",
    "        output is the corresponding label tensor.\n",
    "\n",
    "        Args:\n",
    "            idx: (int) The index of a sequence inside the whole dataset, from\n",
    "                which the function will choose the reference and search frames.\n",
    "\n",
    "        Returns:\n",
    "            ref_frame (torch.Tensor): The reference frame with the\n",
    "                specified size.\n",
    "            srch_frame (torch.Tensor): The search frame with the\n",
    "                specified size.\n",
    "            label (torch.Tensor): The label created with the specified\n",
    "                function in self.label_fcn.\n",
    "        \"\"\"\n",
    "        # Negative sample\n",
    "        if random.random() < self.neg_prob:\n",
    "            is_positive = False\n",
    "        else: \n",
    "            is_positive = True\n",
    "        \n",
    "        # Obtain the video and the index inside that video. Then, read the imae\n",
    "        video_name, idx_first_frame = self.get_data_from_idx(idx)\n",
    "        first_frame = cv2.imread(self.dict_frames_per_video[video_name][idx_first_frame])\n",
    "\n",
    "        # Obtain bounding boxes\n",
    "        bbox1 = self.dict_bboxes_per_video[video_name][idx_first_frame]\n",
    "\n",
    "        if is_positive: # Positive sample\n",
    "            print(\"Positive!\")\n",
    "            second_frame, bbox2 = self.get_positive_sample(video_name, idx_first_frame)\n",
    "            # If the width or height of the box are 0 it is actually a negative sample!!\n",
    "            if bbox2[2]==0 or bbox2[3]==0:\n",
    "                print(\"The sample is actually negative!! Random crop.\")\n",
    "                is_positive = False\n",
    "                # Create a random bbox to patch over it\n",
    "                x = random.randint(0, 0.9*second_frame.shape[1])\n",
    "                y = random.randint(0, 0.9*second_frame.shape[0])\n",
    "                w = random.randint(0.1*second_frame.shape[1], second_frame.shape[1]-x-1)\n",
    "                h = random.randint(0.1*second_frame.shape[0], second_frame.shape[0]-y-1)\n",
    "                bbox2 = [x, y, w, h]\n",
    "\n",
    "        else: # Negative sample\n",
    "            print(\"Negative!\")\n",
    "            second_frame, bbox2 = self.get_negative_sample(video_name)\n",
    "\n",
    "        template, search, bbox1_x1y1wh, bbox2_x1y1wh = self.preprocess_pair(first_frame, second_frame, bbox1, bbox2)\n",
    "\n",
    "        if is_positive:\n",
    "            heatmap, reg_wh = self.make_rect_tent(bbox2_x1y1wh)\n",
    "        else:\n",
    "            reg_wh = np.zeros((self.size_out, self.size_out, 2), dtype=np.float32)\n",
    "            heatmap = np.zeros((self.size_out, self.size_out), dtype=np.float32)\n",
    "\n",
    "        #output = {'template': self.to_tensor(template),\n",
    "        #          'search': self.to_tensor(search),\n",
    "        #          'heatmap': heatmap,\n",
    "        #          'reg_wh': reg_wh}\n",
    "        return template, search, heatmap, reg_wh\n",
    "        #return output\n",
    "        #return first_frame, second_frame, template, search, bbox1_x1y1wh, bbox2_x1y1wh, heatmap, reg_wh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cadb662-8d37-4faa-b5ec-86fa8064f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetLaSOT(\"train\", \"/home/rafa/deep_learning/datasets/LaSOT\", 127, 255, 127, 10, 0.45, 0.5, 0.75, 1.5, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9dd2d57-01fc-4769-b8ce-c87a9e06d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.visualize_video(\"airplane-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3503eff0-4444-4a8b-adfe-1b3b3cacec55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative!\n",
      "With object!\n",
      "bus-10\n",
      "Extra context search: 1.1480946756504888\n",
      "torch.Size([3, 127, 127])\n",
      "torch.Size([3, 255, 255])\n",
      "(127, 127)\n",
      "(127, 127, 2)\n"
     ]
    }
   ],
   "source": [
    "output = dataset.__getitem__(10000)\n",
    "print(output['template'].shape)\n",
    "print(output['search'].shape)\n",
    "print(output['heatmap'].shape)\n",
    "print(output['reg_wh'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d119a1ec-1f42-4a40-8f2e-2824c10c3e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative!\n",
      "Without object!\n",
      "boat-20\n",
      "Extra context search: 1.3938253609638867\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m first_frame, second_frame, template, search, bbox1, bbox2, heatmap, reg_wh \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(first_frame\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(cv2\u001b[38;5;241m.\u001b[39mcvtColor(first_frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 4)"
     ]
    }
   ],
   "source": [
    "first_frame, second_frame, template, search, bbox1, bbox2, heatmap, reg_wh = dataset.__getitem__(10000)\n",
    "print(first_frame.shape)\n",
    "plt.imshow(cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768d5dd-5996-4f0e-8ef8-4244824a6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, w, h = bbox1\n",
    "x1, y1, w, h = int(x1), int(y1), int(w), int(h)\n",
    "cv2.rectangle(template, (x1, y1), (x1+w, y1+h), (0, 255, 0), 2)\n",
    "plt.imshow(cv2.cvtColor(template.astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854228a-1a43-4772-a721-2ceae96d9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, w, h = bbox2\n",
    "x1, y1, w, h = int(x1), int(y1), int(w), int(h)\n",
    "cv2.rectangle(search, (x1, y1), (x1+w, y1+h), (0, 255, 0), 2)\n",
    "plt.imshow(cv2.cvtColor(search.astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d172ce-cb66-482a-be46-90eeea84f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cx: \", int((x1+w/2)/dataset.size_search*dataset.size_out))\n",
    "print(\"cy: \", int((y1+h/2)/dataset.size_search*dataset.size_out))\n",
    "print(np.max(heatmap))\n",
    "cy, cx = np.unravel_index(heatmap.argmax(), heatmap.shape)\n",
    "w, h = reg_wh[cy, cx]\n",
    "#cx = round((x1+w/2)/dataset.size_search*dataset.size_out)\n",
    "#cy = round((y1+h/2)/dataset.size_search*dataset.size_out)\n",
    "print(\"cx: \", cx)\n",
    "print(\"cy: \", cy)\n",
    "print(\"w: \", w)\n",
    "print(\"h: \", h)\n",
    "#cv2.rectangle(heatmap, (int(cx-w*dataset.size_out/2), int(cy-w*dataset.size_out/2)), (int(cx+w*dataset.size_out/2), int(cy+w*dataset.size_out/2)), (0, 255, 0), 2)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(heatmap, cmap='jet', interpolation='bilinear')\n",
    "plt.colorbar(label='Heatmap value')\n",
    "plt.title('Rectangular Gaussian Heatmap')\n",
    "plt.xlabel('Output X')\n",
    "plt.ylabel('Output Y')\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((cx-w*dataset.size_out/2, cy-h*dataset.size_out/2), w*dataset.size_out, h*dataset.size_out, \n",
    "                         linewidth=2, edgecolor='white', facecolor='none')\n",
    "\n",
    "# Add the rectangle to the current axes\n",
    "plt.gca().add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41dd605-342f-44e1-9ea6-f63166f83893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371c8b3-e338-4a23-9235-a71ba8b04d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f9cdd-3115-4d55-bc89-fd10774ba842",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox1_x1y1x2y2 = x1y1wh_x1y1x2y2(*bbox1)\n",
    "first_frame_cropped = dataset.crop_roi(first_frame, bbox1_x1y1x2y2, dataset.size_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a492999-df70-45e9-8e83-ad34dc5e7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(first_frame_cropped, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e8ce8-03b3-47d5-9424-0d9825246d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = bbox1\n",
    "cv2.rectangle(first_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "plt.imshow(cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874b65e-15c1-42bd-a386-719415f1163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped = first_frame[y:y+h, x:x+w]\n",
    "cropped = cv2.resize(cropped, (dataset.size_template, dataset.size_template))\n",
    "plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2dc8e9-a5bf-4579-b3b6-c17ff5cf767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a932824-53bf-4478-ba58-5b2ff3d90d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
